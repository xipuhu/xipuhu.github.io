---
title: 03卷积神经网络反向传播：从直觉到推导
categories: [计算机视觉,吴恩达深度学习]
tags: [深度学习]
mathjax: true
toc: true
---

### 卷积层反向传播理解

#### 从全连接层看卷积层

##### 全连接层的反向传播过程

　　在CNN中，由于增加了卷积运算，相较于全连接层神经网络（也就是我们之前学过的DNN），其理解起来更加的有难度了。在CNN的前向传播过程中，理解了卷积运算后，基本上对整个前向传播过程的理解并不会有太大困难，但是对于CNN的反向传播过程的理解，还需要下一番功夫，主要的原因还是在于引入了卷积运算，接下来就开始对CNN的反向传播做进一步的加深理解。

<!--more-->

　　首先，我们需要将DNN的反向传播重温下，其过程中涉及到的数学推导如下：

$input:da^{[l]}$

$output: da^{[l-1]}，dW^{[l]}，db^{[l]}$

　　相关公式：

$dz^{[l]}=da^{[l]} \odot g^{[l]}{’}(z^{[l]})\tag{1}$

$dW^{[l]}=dz^{[l]}\odot a^{[l-1]} \tag{2}$

$ db^{[l]}=dz^{[l]}\tag{3}$

$da^{[l-1]}=W^{[l]}{^T}\odot dz^{[l]}\tag{4}$  

　　由公式(1)和公式(4)，我们还可以推导出如下公式：

$dz^{[l]}=dz^{[l+1]} \odot W^{[l]} \odot g^{[l]}{’}(z^{[l]})\tag{5}$ 

##### 卷积层的反向传播公式

　　由在网上对CNN反向传播相关资料的说明，我们可以知道有：

$\delta^{l-1}=\delta ^{l}\frac{\partial z^{l}}{\partial z^{l-1}}=\delta ^{l}\ast rot180(W^{l})\odot\sigma ^{’}(z^{l-1}) \tag{6}$ 

　　**注意：** 在上式中， $\delta^{l}=\frac{\partial J(W,b)}{\partial z^l}=dz^{l}$  ，这样写是为了和下面的公式推导保持一致。

　　相信你已经注意到了相较于全连接层中的公式，该式子中居然只是将矩阵乘法换成了卷积符号，并且将$W^l$ 进行了一个180度翻转。对于这一个操作你可能会产生一个疑惑，其实在网上的大部分相关资料都没有对为什么要将$W^l$ 作一个180度翻转后再进行卷积做出一个明确的解释，这也是接下来主要的内容说明。

##### 将全连接层转换为卷积层

　　我们如何将全连接层和卷积层联系起来呢？答案如下图所示：

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%90%86%E8%A7%A3/01.png" width=60% height=60%>


　　在全连接层中切断一些连接，并使之权重共享后，便得到了最右边的卷积层！为了更进一步的理解可以再看看下面这张图：

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%90%86%E8%A7%A3/02.png" width=60% height=60%>

　　在上图中进行卷积之前，我们注意到先对$W^l$ 进行了一个180度翻转，其实在数学上卷积运算也称为互相关：

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%90%86%E8%A7%A3/03.png" width=60% height=60%>

　　在数学上，卷积之前还会有一个镜像操作，即进行一个180度的翻转，有时候被称为`互相关` ，而不是`卷积` 。对于为什么需要这样操作，我们接着往下看，需要注意一点的是，接下来的数学推导都是从数学上卷积开始的，也就是在卷积之前，我们需要对过滤器进行一个180度的旋转，但是在实际使用CNN的过程中，深度学习会跳过这个镜像操作，这在深度学习中已经是一个惯例了。

#### 卷积层反向传播

　　现在，我们将注意力集中在CNN的反向传播计算上面来，

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%90%86%E8%A7%A3/04.png" width=60% height=60%>


　　其详细计算过程如下图所示：

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%90%86%E8%A7%A3/05.png" width=60% height=60%>

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%90%86%E8%A7%A3/06.png" width=60% height=60%>

　　上面卷积运算过程和前面CNN前向传播的卷积运算有一点不同，在这里我们的卷积运算是`valid卷积运算 `，即通过这次的卷积运算后，图片的大小是会发生改变的，在前向传播中，图片的尺寸会变小，而在反向传播中我们则会使图片尺寸变大，其实很容易理解，也就是一个逆的过程。注意了，上面的运算，都是基于$rot180(W)$ 而来的。
　　**注意：** 在前向传播和反向传播中，其矩阵点积的运算都被替换为了卷积运算。

　　接下来，将通过公式推导的过程来证明，为什么在反向传播过程中可以将$W^l$ 进行一个180度的翻转，从而参与卷积运算。

##### 互相关和卷积

　　在全连接层神经网络中，我们可以将神经元$j$ 的误差定义为：

$\delta_j^{l}=\frac{\partial C}{\partial z_j^l} \tag{7}$

 　　其中，$z_j^l=\sum\limits_{k}w_{jk}^la_k^{l-1}+b_j^l=\sum\limits_{k}w_{jk}^l\sigma(z_j^l)+b_j^l$  

　　但是，在卷积神经网络中，由于矩阵乘法已经被之前讨论的卷积给替代了，这里，我们将不再计算$z_j$ 的梯度了，而是对$z_{x,y}$ 计算梯度：

　　$z_{x,y}^{l+1}=w^{l+1}\ast \sigma(z_{x^{’},y^{’}}^{l})+b_{x,y}^{l+1}=\sum\limits_{a=0}^{f-1}\sum\limits_{b=0}^{f-1}w_{a,b}^{l+1}\sigma(z_{x^{’}-a,y^{’}-b}^l)+b_{x,y}^{l+1}\tag{8}$

　　**对公式(8)的补充说明：** 其中$f$ 表示的是过滤器的大小，即$w$ 的维度，在上述计算过程中$w$ 的下标是从$(0,0)$ 开始了，实际过程中$w$ 的下标应为$(a+1,b+1)$ ，之所以需要这样是为了在公式推导中能够方便的描述，并且公式中$x^{’}$ 和$y^{’}$ 的大小必须至少从$f$ 开始。
　　对于$\sigma(z_{x^{’}-a,y^{’}-b}^l)$ 的下标有如下描述：
　　由于在卷积之前对了过滤器进行了一个镜像操作，所以下标为$(x^{'}-a,y^{’}-b)$ ，在实际使用CNN中，我们跳过了镜像操作，所以在实际操作中，该公式应该为：

$z_{x,y}^{l+1}=w^{l+1}\ast \sigma(z_{x^{’},y^{’}}^{l})+b_{x,y}^{l+1}=\sum\limits_{a=0}^{f-1}\sum\limits_{b=0}^{f-1}w_{a,b}^{l+1}\sigma(z_{x^{’}+a,y^{’}+b}^l)+b_{x,y}^{l+1}\tag{9}$ 

　　**镜像操作举例说明：** 

　　比如在上面的例子中，$f=2$ ，由对公式(8)的补充说明，我们可以知道$x^{’}\leq 2$ ，$y^{’}\leq 2$，这里我们假设$x^{’}= 2$，$y^{’}=2$ :
　　当$a=0,b=0$ 时，则在公式(8)中，有$w_{0,0}^{l+1}\sigma(z_{2,2}^l)$ ，即实际计算中为$w_{1,1}^{l+1}\sigma(z_{2,2}^l)$ ，这个结果其实就是一个镜像操作的结果，我们知道，如果没有镜像操作的话，其结果应该是$w_{1,1}^{l+1}\sigma(z_{1,1}^l)$ ，这个结果可以通过公式(9)计算而来。

##### rot180(W)的推导证明

　　下面将对“在CNN反向传播计算梯度的时候，为什么要将$W$ 进一个翻转”这个问题，进行推导证明：

　　我们先从以下公式开始：

$\delta_{x,y}^l=\frac{\partial C}{\partial z_{x,y}^l}=\sum\limits_{x^{’}}\sum\limits_{y^{’}}\frac{\partial C}{\partial z_{x^{’},y^{’}}^{l+1}}\frac{\partial z_{x^{’},y^{’}}^{l+1}}{\partial z_{x,y}^l}\tag{10}$ 

 　　根据求导的链式法则，我们可以对公式(10)做进一步的处理：

$\delta_{x,y}^l=\frac{\partial C}{\partial z_{x,y}^l}=\sum\limits_{x^{’}}\sum\limits_{y^{’}}\frac{\partial C}{\partial z_{x^{’},y^{’}}^{l+1}}\frac{\partial z_{x^{’},y^{’}}^{l+1}}{\partial z_{x,y}^l}=\sum\limits_{x^{’}}\sum\limits_{y^{’}}\delta_{x^{’},y^{’}}^{l+1}\frac{\partial\big(\sum\limits_{a}\sum\limits_{b}w_{a,b}^{l+1}\sigma(z_{x^{’}-a,\ y^{’}-b}^l \  \  )+b_{x^{’},y^{’}}^{l+1}\big)}{\partial z_{x,y}^{l}}\tag{11}$ 

 　　　　　　　　　$s.t. \ \ \ \ \ \ x = x^{’}-a \ \ and \ \ y=y^{’}-b$ 

　　由于$x=x^{’}-a$ 并且$y=y^{’}-b$ ，所以：

$\sum\limits_{x^{’}}\sum\limits_{y^{’}}\delta_{x^{’},y^{’}}^{l+1}\frac{\partial\big(\sum\limits_{a}\sum\limits_{b}w_{a,b}^{l+1}\sigma(z_{x^{’}-a,\ y^{’}-b\ \ }^{l})+b_{x^{’},y^{’}}^{l+1}\big)}{\partial z_{x,y}^{l}}=\Big(\sum\limits_{x^{’}}\sum\limits_{y^{’}}\delta_{x^{’},y^{’}}^{l+1}w_{a,b}^{l+1}\Big)\odot\sigma^{’}({z_{x,y}^l})\tag{12}$ 

　　由于$a=x^{’}-x$ 并且$b=y^{’}-y$ ，所以：

$\sum\limits_{x^{’}}\sum\limits_{y^{’}}\delta_{x^{’},y^{’}}^{l+1}\frac{\partial\big(\sum\limits_{a}\sum\limits_{b}w_{a,b}^{l+1}\sigma(z_{x^{’}-a,\ y^{’}-b\ }^{l})+b_{x^{’},y^{’}}^{l+1}\big)}{\partial z_{x,y}^{l}}=\Big(\sum\limits_{x^{’}}\sum\limits_{y^{’}}\delta_{x^{’},y^{’}}^{l+1}w_{x^{’}-x,y^{’}-y}^{l+1}\Big)\odot \sigma^{’}({z_{x,y}^l})\tag{13}$ 

　　因此最终得到的公式为：

$\delta_{x,y}^l=\sum\limits_{x^{’}}\sum\limits_{y^{’}}\delta_{x^{’},y^{’}}^{l+1}\frac{\partial\big(\sum\limits_{a}\sum\limits_{b}w_{a,b}^{l+1}\sigma(z_{x^{’}-a,\ y^{’}-b\ \  })+b_{x^{’},y^{’}}^{l+1}\big)}{\partial z_{x,y}^{l}}=\Big(\sum\limits_{x^{’}}\sum\limits_{y^{’}}\delta_{x^{’},y^{’}}^{l+1}w_{-x,-y}^{l+1}\Big)\odot\sigma^{’}({z_{x,y}^l})\tag{14}$ 

​	为什么需要对权重$w$ 进行一个翻转呢？其实在公式(14)中，有：

$ROT180(w_{x,y}^{l+1})=w_{-x,-y}^{l+1}\tag{15}$  

　　由上述的推导过程，我们可知，其实对权重$w$ 的一个翻转，是由在反向传播中对误差$\delta$ 进行推导而来的。好了，最后还有一个反向传播计算中比较重要的组成，便是计算权重的梯度$\frac{\partial C}{\partial w_{a,b}^{l}}$ ：

$$ \frac{\partial C}{\partial w_{a,b}^{l}} =\sum\limits_{x}\sum\limits_{y}\frac{\partial C}{\partial z_{x,y}^{l}}\frac{\partial z_{x,y}^l }{\partial w_{a,b}^{l}}$$

　　　　　　　　　　　　　　　　　　 　　　　$= \sum\limits_{x}\sum\limits_{y} \delta_{x,y}^{l}\frac{\partial\big(\sum\limits_{a^{’}}\sum\limits_{b^{’}}w_{a^{’},b^{’}}^{l}\sigma(\ z_{x-a^{’},y-b^{’}}^{l-1}\ \  )+b_{x,y}^{l}\big)}{\partial w_{a,b}^{l}}$    

　　　　　　　　　　　　　　　　　　　　　　$=\sum\limits_{x}\sum\limits_{y} \delta_{x,y}^l \sigma (z_{x-a,y-b}^{l-1})$    

　　　　　　　　　　　　　　　　　　　　　　$=\delta_{a,b}^l \ast \sigma(z_{-a,-b}^{l-1})$      

　　　　　　　　　　　　　　　　　　　　　　$=\delta_{a,b}^l \ast \sigma(ROT180(z_{a,b}^{l-1}))$

　　**对公式(13)的补充说明：**  

　　在该公式结果中，第一部分是$ \sum\limits_{x^{’}}\sum\limits_{y^{’}}\delta_{x^{’},y^{’}}^{l+1}w_{x^{’}-x,y^{’}-y}^{l+1}$ ，其中$w^{l+1}$ 的下标表示再一次对$w^{l+1}$ 进行了一次镜像操作，所以在计算中，实际上对$w^{l+1}$ 总共进行了两次的镜像操作：

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%90%86%E8%A7%A3/07.png" width=60% height=60%>

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%90%86%E8%A7%A3/08.png" width=60% height=60%>

　　由于对$w^{l+1}$ 总共进行了两次的镜像操作，所以在实际计算过程中，我们可以从上图中知道，两次镜像操作的效果其实抵消了，即$w^{l+1}$ 并没有因为镜像操作而发生了变化，所以参与卷积运算的$w^{l+1}$ 和原来的$w^{l+1}$ 一样。
　　**注意：** 卷积运算在深度学习实际使用中，由于在前向传播的时候跳过了一次镜像操作，所以卷积核只在反向传播的计算中进行了一次镜像操作，上面的说明中之所以会有两次的镜像操作，是因为在上述推导中，我们是从数学上的卷积运算（也被称为互相关)开始的。

　　$ \sum\limits_{x^{’}}\sum\limits_{y^{’}}\delta_{x^{’},y^{’}}^{l+1}w_{x^{’}-x,y^{’}-y}^{l+1}$ 中由于$x=x^{’}-a，y=y^{’}-b$ 的条件限制，所以$0\leq x \leq2，0\leq y \leq2$ ，在上图中，我们还发现，在计算过程中对$\delta_{x^{’},y^{’}}^{l+1}$ 进行了一个$Padding$ 操作，这是因为，在$w^{l+1}$ 其维数大小$f=2$ ，$x^{’},y^{’}$ 的大小必须要从2开始，因此对$\delta_{x^{’},y^{’}}^{l+1}$ 进行了一个$pad=f-1=1$ 的$Padding$ 操作。

### 反向传播过程的池化层操作

　　在池化层操作中，我们由CNN的前向传播过程可以知道，在池化层中并没有进行任何学习，因此在反向传播的计算中，池化层的操作会简单很多。
 　　在反向传播时，我们首先会把$\delta ^{l}$的所有子矩阵大小还原成池化之前的大小，然后如果是$MAX$ ，则把的所有子矩阵的各个池化局域的值放在之前做前向传播算法得到最大值的位置。如果是$Average$ ，则把δl的所有子矩阵的各个池化局域的值取平均后放在还原后的子矩阵位置。这个过程一般叫做`upsample` 。
　　用一个例子可以很方便的表示：假设我们的池化区域大小是$2\times2$ 。$\delta ^{l}$的第$k$ 个子矩阵为: 

$$\delta_k^l = \left( \begin{matrix} 2 & 8 \\\ 4 & 6 \end{matrix} \right)$$ 

　　由于池化区域为$2\times2$，我们先讲$\delta ^{l}_k$做还原，即变成：

$$\left( \begin{array}{cccc} 0&0&0&0 \\\ 0&2& 8&0 \\\ 0&4&6&0 \\\ 0&0&0&0 \end{array} \right)$$ 

　　如果是$Max$ ，假设我们之前在前向传播时记录的最大值位置分别是左上，右下，右上，左下，则转换后的矩阵为：

$$\left( \begin{array}{ccc} 2&0&0&0 \\\ 0&0& 0&8 \\\ 0&4&0&0 \\\ 0&0&6&0 \end{array} \right)$$ 

　　如果是$Average$ ，则进行平均转换后的矩阵为:

$$\left( \begin{array}{ccc} 0.5&0.5&2&2 \\\ 0.5&0.5&2&2 \\\ 1&1&1.5&1.5 \\\ 1&1&1.5&1.5 \end{array} \right)$$

　　由上面的举例说明，我们可以概括下张量$\delta ^{l-1}$ ：

$$\delta^{l-1} =  upsample(\delta^l) \odot \sigma^{’}(z^{l-1})$$ 

### 小结

对于CNN的反向传播算法，有如下步骤总结：

1.输入$X$：并为输入层设置相应的激活函数

2.前向传播：
$for \  each \  l=2,3\cdots,L:$

   　　　　$Compute:  z_{x,y}^l=w^l\ast\sigma(z_{x,y}^{l-1})+b_{x,y}^l $  　$and$　    $a_{x,y}^l=\sigma(z_{x,y}^l)$    

3.输出误差$\delta^L$ :  $\delta^L=\frac{\partial C}{\partial a}\odot \sigma^{’}(z^L)$
​

4.对误差进行反向传播计算：
$for \  each \  l=L-1,L-2,\cdots,2 :$

   　　　　$Compute: \delta_{x,y}^l=\big(\delta^{l+1}\ast ROT180(w_{x,y}^{l+1})\big)\odot \sigma^{’}(z_{x,y}^l)$   

5.输出损失函数$C$ 对$w$的梯度：$\frac{\partial C}{\partial w_{a,b}^{l}} =\delta_{a,b}^l \ast \sigma\big(ROT180(z_{a,b}^{l-1})\big)$ 

-------------------------

参考：https://grzegorzgwardys.wordpress.com/2016/04/22/8/#unique-identifier2
　　　https://www.cnblogs.com/pinard/p/6494810.html
　　　http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/