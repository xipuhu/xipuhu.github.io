---
title: 06深度学习的实践方面
categories: [计算机视觉,吴恩达深度学习]
tags: [深度学习]
mathjax: true
toc: true
---

### 训练集、验证集、测试集

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E8%B7%B5%E6%96%B9%E9%9D%A2/01.png" style="zoom:67%;" />

<!--more-->

　　对于一个需要解决的问题的样本数据，在建立模型的过程中，需要将问题的data划分为以下几个部分：

* 训练集 ：用训练集对算法或模型进行训练过程；
* 验证集 ：利用验证集(简单交叉验证集)进行交叉验证，对模型做最后的调整，然后选择出最好的模型；
* 测试集 ：最后利用测试集对模型进行测试，获取模型运行的无偏估计；

#### 小数据量

　　若是小数据量的情况，比如：100，1000，10000的数据量大小，则可以将**data** 做以下划分：

* 无验证集的情况：70% / 30%
* 有验证集的情况：60% / 20% / 20%

#### 大数据量

　　若拥有**data** 的数量是百万级别的，验证集和测试集所占的比重将会趋向于变得更小：

* 100万数据量：98% / 1% / 1%
* 超百万数据量：99.5% / 0.4% / 0.4%

　　**注意：** 1、验证集和训练集来自同一分布，可以使算法变得更快 2、如果不需要用无偏差估计来评估
　　　　　模型的性能，则可以不需要测试集。

### 高偏差(欠拟合)和高方差(过拟合)

　　高偏差和高方差如下图所示：

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E8%B7%B5%E6%96%B9%E9%9D%A2/02.png" style="zoom:67%;" />

#### 判别是高偏差还是高方差

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E8%B7%B5%E6%96%B9%E9%9D%A2/03.png" style="zoom:67%;" />

　　对于高偏差和高方差，我们可以通过训练集误差和验证集误差之间大小的比较，来进行判断：

* **高偏差情况(欠拟合)** ：训练集的错误率很高(正确率很低)，而验证集的错误率和它相差不大；
* **高方差情况(过拟合)** ：训练集的错误很低(正确率很高)，而验证集的错误率比它要高好多(正确率要低好多)；
* **要得到的效果情况(低偏差，低方差)**  ：训练集的错误很低(正确率很高)，而验证集的错误率和它相差不大；

#### 对高偏差和高方差的处理方法

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E8%B7%B5%E6%96%B9%E9%9D%A2/04.png" style="zoom:67%;" />

出现**高偏差(High bias)** ，可以采取以下措施：

* 选择一个新网络，比如有更多的隐藏层或隐藏层单元；
* 花费更多的时间去训练。

出现**高方差(High variance)** ，可以采取以下措施：

* 采用更多的数据；
* 进行正则化操作。

### 正则化

#### $l_2$ 正则化

　　利用正则化可以解决高方差(High variance)，即过拟合的问题，在Cost funcation中加入一项正则化项，起到惩罚模型复杂度的效果，从而来减弱过拟合。

##### Logistic regression

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E8%B7%B5%E6%96%B9%E9%9D%A2/05.png" style="zoom:67%;" />

　　加入正则化项的代价函数：

$$J(w,b)=\dfrac{1}{m}\sum\limits_{i=1}^{m}l(\hat y^{(i)},y^{(i)})+\dfrac{\lambda}{2m}||w||_{2}^{2}$$ 

　　上式为逻辑回归的$l_2$ 正则化，其中$\lambda$ 为正则化因子。

##### Neural network

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E8%B7%B5%E6%96%B9%E9%9D%A2/06.png" style="zoom:67%;" />

　　加入正则化项的代价函数：
$$J(w^{[1]},b^{[1]},\cdots,w^{[L]},b^{[L]})=\dfrac{1}{m}\sum\limits_{i=1}^{m}l(\hat y^{(i)},y^{(i)})+\dfrac{\lambda}{2m}\sum\limits_{l=1}^{L}||w^{[l]}||_{F}^{2}$$ 

　　其梯度变为：
$$dW^{[l]} = (form\_backprop)+\dfrac{\lambda}{m}W^{[l]}$$ 

　　梯度更新公式变为：
$$W^{[l]}:= W^{[l]}-\alpha dW^{[l]}$$ 

　　代入可得：
$$W^{[l]}:= W^{[l]}-\alpha [ (form\_backprop)+\dfrac{\lambda}{m}W^{[l]}]$$

$$\ \ \  \ \ \  = W^{[l]}-\alpha\dfrac{\lambda}{m}W^{[l]} -\alpha(form\_backprop)$$

$$\ \ \  =(1-\dfrac{\alpha\lambda}{m})W^{[l]}-\alpha(form\_backprop)$$

　　其中，$(1-\frac{\alpha \lambda}{m})<1$ ，会给原来的$W^{[l]}$ 一个衰减的参数，所以$l_2$ 范数正则化也被称为“权重衰减”。

##### 正则化能够减小过拟合的原因

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E8%B7%B5%E6%96%B9%E9%9D%A2/07.png" style="zoom:67%;" />

　　通过增加$\lambda$ 的值，我们可以将$W^{[l]}$ 的值减少，即神经网络中的一些结点的权重会减小，假如让$W^{[l]}$ 一直减小并趋于零，网络中就会相当于少了很多相应的结点，最后整个网络结构变得更简单了，训练集的错误率将会增加，从而使得高方差状态向高偏差状态转移。

##### 正则化能够减小过拟合的数学解释

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E8%B7%B5%E6%96%B9%E9%9D%A2/08.png" style="zoom:67%;" />

#### Dropout(随机失活)正则化

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E8%B7%B5%E6%96%B9%E9%9D%A2/09.png" style="zoom:67%;" />

　　Dropout（随机失活）就是在神经网络的Dropout层，为每个神经元结点设置一个随机消除的概率，对于保留下来的神经元，我们得到一个节点较少，规模较小的网络进行训练。

##### 实现Dropout

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E8%B7%B5%E6%96%B9%E9%9D%A2/10.png" style="zoom:67%;" />

　其实现的主要代码内容如下：

```python
keep_prob = 0.8  # 设置神经元保留概率
d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep_prob  # 返回一个布尔型矩阵
a3 = np.multiply(a3, d3)	# a3通过与d3的相乘来决定消除该层网络的哪些结点
a3 /= keep_prob	# 弥补a3期望值的减少
```

##### 使用Dropout的注意事项

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E8%B7%B5%E6%96%B9%E9%9D%A2/11.png" style="zoom:67%;" />

　　**缺点：** Dropout的一大缺点就是，代价函数$J$ 不再被明确的定义了，还有就是为了使用交叉验证，需要搜索更多的超参数。

　　**使用Dropout：** 

  * 先关闭Dropout功能，即设置keep-prob=1;
  * 再运行代码，确保损失函数单调递减；
  * 最后再打开Dropout函数。

　　**注意：** 不要在测试阶段使用Dropout，因为这样会使得预测结果变得随机。
　　**对于keep-prob的说明：** 如果担心某些层比其他层更容易发生过拟合，可以把这些层的keep-prob值设置比其他层更低，即keep-prob值越低，结点消除的可能性就越大。

##### Early Stopping

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E8%B7%B5%E6%96%B9%E9%9D%A2/12.png" style="zoom:67%;" />

### 归一化输入

#### 使用归一化

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E8%B7%B5%E6%96%B9%E9%9D%A2/13.png" style="zoom:67%;" />

　　对数据集特征$x_1$ ，$x_2$ 归一化的步骤如下：

1. 计算每个特征所有样本数据的均值：$\mu = \dfrac{1}{m}\sum\limits_{i=1}^{m}x^{(i)}$ 
2. 减去均值得到对称的分布：$x : =x-\mu$ 
3. 归一化方差：$\sigma^{2} = \dfrac{1}{m}\sum\limits_{i=1}^{m}x^{(i)^{2}}$ ，$x=x/\sigma^2$ 

#### 归一化的原因

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E8%B7%B5%E6%96%B9%E9%9D%A2/14.png" style="zoom:67%;" />

### 梯度爆炸和梯度消失

#### 梯度爆炸和梯度消失的原因

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E8%B7%B5%E6%96%B9%E9%9D%A2/15.png" style="zoom:67%;" />

　　如上图所示，如果梯度呈指数型发生改变，就会很容易出现梯度爆炸或梯度消失的结果，而且随着网络层数的增加，其产生的效果越明显。

#### 防止梯度爆炸或消失的方法

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E8%B7%B5%E6%96%B9%E9%9D%A2/16.png" style="zoom:67%;" />

　　我们可以通过改变权重$W^{[l]}$ 的值，来防止梯度因为改变幅度过大而发生这两种现象：

  * 激活函数使用ReLU的时候: $W^{[l]}=np.random.randn(shape)*np.sqrt(\frac{2}{n^{[l-1]}}\ )$ 
  * 激活函数使用tanh的时候：$W^{[l]}=np.random.randn(shape)*np.sqrt(\frac{1}{n^{[l-1]}}\ )$ 
  * 当激活函数是其他函数的时候：$W^{[l]}=np.random.randn(shape)*np.sqrt(\frac{2}{n^{[l-1]}+n^{[l]}})$ 

### 梯度检验

#### 梯度的数值逼近

　　我们可以使用双边误差的方法(求导的定义法)，来逼近导数：
<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E8%B7%B5%E6%96%B9%E9%9D%A2/17.png" style="zoom:67%;" />

　　其中用到的数学公式主要如下：
$$f'(\theta) = \lim\limits_{\varepsilon  \to 0}=\dfrac{f(\theta+\varepsilon)-(\theta-\varepsilon)}{2\varepsilon}$$ 

#### 梯度检验的步骤

##### 连接参数

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E8%B7%B5%E6%96%B9%E9%9D%A2/18.png" style="zoom:67%;" />

　　因为在神经网络中含有大量的参数：$W^{[1]},b^{[1]},\cdots,W^{[L]},b^{[L]}$ ，为了做梯度检验，我们需要将这些参数全部连接起来，reshape成一个大的向量$\theta$ ，因为在代码中，上述参数都是以字典的形式存在的，因此需要转换成向量来进行处理，其中会涉及到两个主要的操作：字典转向量、向量转字典。

##### 进行梯度检验

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E8%B7%B5%E6%96%B9%E9%9D%A2/19.png" style="zoom:67%;" />

　　判断$d\theta_{approx}\approx d\theta$ 是否接近，首先利用双边误差的方法计算出$d\theta_{approx}$ ，然后在通过求导计算出$d\theta$ ，最后通过相应公式求出其两者之间的差值，比较差值得出梯度值的好坏，涉及到的公式如下：

　　求$d\theta_{approx}$ :
$$d\theta_{approx} = \lim\limits_{\varepsilon  \to 0}=\dfrac{f(\theta+\varepsilon)-(\theta-\varepsilon)}{2\varepsilon}$$ 

　　**注意：** $\varepsilon$ 一般取值$10^{-7}$   

　　判断公式：

$$\frac{\Arrowvert d \theta_{approx}-d \theta\Arrowvert_2}{\Arrowvert d \theta_{approx}\Arrowvert _2 + \Arrowvert d \theta\Arrowvert _2}$$ 

　　如果上述公式的计算结果小于$10^{-7}$ ，则说明梯度值没有问题，如果结果大于$10^{-3}$ ，则需要考虑是否存在bug了。

#### 使用梯度检验注意事项

* 不能在训练期间进行梯度检验(梯度检验的计算代价太大)，只在调试的阶段使用梯度检验；
* 梯度检验不能与Dropout同时使用，因为每次迭代的过程中，Dropout会随机消除隐藏层单元的不同神经元，这时难以计算Dropout在梯度上的代价函数$J$ ；

参考自:   https://blog.csdn.net/koala_tree/article/details/78125697
代码链接：https://pan.baidu.com/s/1T0mKgzTYs44Fjh6Mf8MYbQ 密码：dytw