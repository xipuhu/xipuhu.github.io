---
title: 02神经网络和深度学习——深层神经网络
categories: [计算机视觉,吴恩达深度学习]
tags: [深度学习]
mathjax: true
toc: true
---

### 参数W、b、Z、A的矩阵维数问题

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/01.png" style="zoom:67%;" />

<!--more-->

 　　符号约定：$n^{[l]}$ 表示第$l$ 层网络的单元数，故对于第$l$ 层神经网络，其各个参数的矩阵维度为：

* $W^{[l]}:\ (n^{[l]}, n^{[l-1]})$ 
* $b^{[l]}\: (n^{[l]}, 1)$ 
* $dW^{[l]} :\ (n^{[l]}, n^{[l-1]})$ 
* $db^{[l]} : (n^{[l]}, 1)$ 
* $Z^{[l]}: (n^{[l]},1)$
* $A^{[l]} = Z^{[l]}: (n^{[l]},1)$ 

　　**注意：** 其中输入层作为整个网络的第0层，并且$X=a^{[0]}$ ，$\hat{y}=a^{[L]}$ 

　　各个参数之间的关系如下面公式所示，$g^{[l]}$ 表示的是第$l$ 层的激活函数。

$$z^{[l]} = W^{[l]}a^{[l-1]}+b^{[l]}$$  

$$a^{[l]}=g^{[l]}(z^{[l]})$$  

### 前向传播和反向传播的详细过程

#### 前向传播（Forward propagation）

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/02.png" style="zoom:67%;" />

　　$input:a^{[l-1]}$ 

　　$output: a^{[l]}， cache(z^{[l]})$ 

　　公式：

$$z^{[l]} = W^{[l]}a^{[l-1]}+b^{[l]}$$  

$$a^{[l]}=g^{[l]}(z^{[l]})$$  

　　向量化程序：

$$Z^{[l]}=W^{[l]}\ast A^{[l-1]}+b^{[l]}$$

$$A^{[l]}=g^{[l]}(Z^{[l]})$$

#### 反向传播（Backward propagation）

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/03.png" style="zoom:67%;" />

　　$input:da^{[l]}$

　　$output: da^{[l-1]}，dW^{[l]}，db^{[l]}$

　　公式：

$$dz^{[l]}=da^{[l]} * g^{[l]}{’}(z^{[l]})$$

$$dW^{[l]}=dz^{[l]}\ast a^{[l-1]}=dz^{[l]}\ast g(z^{[l-1]})$$ 

$$ db^{[l]}=dz^{[l]}$$

$$da^{[l-1]}=W^{[l]}{^T}\ast dz^{[l]}$$ 

　　向量化程序：

$$dZ^{[l]}=dA^{[l]} * g^{[l]}{’}(Z^{[l]})$$

$$dW^{[l]}=\dfrac{1}{m}dZ^{[l]}\ast A^{[l-1]}$$

$$db^{[l]}=\dfrac{1}{m}np.sum(dZ^{[l]},axis=1,keepdims = True)$$

$$dA^{[l-1]}=W^{[l]}{^T}\ast dZ^{[l]}$$ 

#### 详细过程截图

<img src="https://hexo-blog-1258021165.cos.ap-guangzhou.myqcloud.com/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/04.png" style="zoom:67%;" />

### 参数和超参数

#### 参数

　　参数指的是在神经网络中想要模型学习到的信息：$W^{[l]}$ ，$b^{[l]}$ 

#### 超参数

　　超参数指的是在神经网络中控制参数输出值的一些网络信息，比如：

* 学习率：$\alpha$
* 迭代次数：$N$
* 隐藏层的层数：$L$
* 每一层的神经元个数：$n^{[1]}, n^{[2]},\cdots$ 
* 激活函数$g(z)$的选择

参考自：https://blog.csdn.net/koala_tree/article/details/78087711